{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify if question is relevant to be stored in Long-Term-Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeQuestion(BaseModel):\n",
    "    \"\"\"Boolean value to check whether a question is related to the specified topics.\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"Is the question relevant? Respond with 'Yes' or 'No'.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are a classifier that examines the given question or statement for any personal information or preferences.\n",
    "Your job is to determine whether the input contains:\n",
    "1. Personal information about the user (e.g., name, occupation, location, contact details, or other identifiable information).\n",
    "2. Preferences, habits, or any explicitly mentioned likes/dislikes.\n",
    "\n",
    "If you find any such information, respond with 'Yes'. If the input does not contain any personal information or preferences, respond with 'No'.\n",
    "Your response must be ONLY 'Yes' or 'No'.\n",
    "\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Here is the input: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "structured_llm = llm.with_structured_output(GradeQuestion)\n",
    "grader_llm = grade_prompt | structured_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_llm.invoke({\"question\": \"Where is Thomas Müller from?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_llm.invoke({\"question\": \"Where is Thomas Müller from? I love playing football myself\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarise question/information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Where is Thomas Müller from? I love playing football myself.\"\n",
    "\n",
    "system = \"\"\"\n",
    "You are an extractor focused on identifying and summarizing personal information from the given input.\n",
    "Personal information includes:\n",
    "1. Names of individuals.\n",
    "2. Locations.\n",
    "3. Hobbies, preferences, or habits explicitly mentioned by the user.\n",
    "4. Any other identifiable personal details.\n",
    "\n",
    "Your task is to:\n",
    "- Extract only the personal information present in the input.\n",
    "- Ignore any irrelevant or general information.\n",
    "- Provide the extracted personal information as a concise, single-sentence summary.\n",
    "\n",
    "If no personal information is found, respond with 'No personal information found.'\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", f\"Extract and summarize personal information: {message}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "summarizer = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the summarizer and print the result\n",
    "result = summarizer.invoke({})\n",
    "print(f\"Extracted Personal Information: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, TypedDict, Sequence\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "import uuid\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "store = InMemoryStore()\n",
    "USER_ID = \"user-123\"\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    personal_info_detected: str\n",
    "    personal_info_extracted: str\n",
    "    is_duplicate: str\n",
    "    collected_memories: str\n",
    "\n",
    "#\n",
    "# 1) Klassifizierer: Enthält die Nachricht persönliche Infos? (Ja/Nein)\n",
    "#\n",
    "class GradeQuestion(BaseModel):\n",
    "    score: str = Field(description=\"Yes/No. Does the user's message contain personal info?\")\n",
    "\n",
    "def personal_info_classifier(state: AgentState) -> AgentState:\n",
    "    message = state[\"messages\"][-1].content\n",
    "    system_prompt = \"You are a classifier. If the message contains personal info, respond 'Yes', otherwise 'No'.\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{message}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    structured_llm = llm.with_structured_output(GradeQuestion)\n",
    "    chain = prompt | structured_llm\n",
    "    result = chain.invoke({\"message\": message})\n",
    "    state[\"personal_info_detected\"] = result.score.strip()\n",
    "    return state\n",
    "\n",
    "def personal_info_router(state: AgentState) -> Literal[\"extract_personal_info\",\"retrieve_memories\"]:\n",
    "    # Falls \"Yes\", extrahieren. Sonst direkt Memories laden.\n",
    "    if state[\"personal_info_detected\"].lower() == \"yes\":\n",
    "        return \"extract_personal_info\"\n",
    "    return \"retrieve_memories\"\n",
    "\n",
    "#\n",
    "# 2) Extrahieren, wenn \"Yes\"\n",
    "#\n",
    "def personal_info_extractor(state: AgentState) -> AgentState:\n",
    "    message = state[\"messages\"][-1].content\n",
    "    extractor_system = \"You are an extractor. Summarize personal info in one sentence.\"\n",
    "    extractor_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", extractor_system),\n",
    "            (\"human\", \"Input: {message}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    chain = extractor_prompt | llm\n",
    "    extracted_info = chain.invoke({\"message\": message})\n",
    "    state[\"personal_info_extracted\"] = extracted_info.strip()\n",
    "    return state\n",
    "\n",
    "#\n",
    "# 3) NEUE Node: Prüft, ob dieses extrahierte Info \"neu\" ist.\n",
    "#    Falls neu -> \"Yes\", sonst -> \"No\".\n",
    "#\n",
    "class InfoNoveltyGrade(BaseModel):\n",
    "    score: str = Field(description=\"Yes/No. Is the new info something we have NOT seen yet?\")\n",
    "\n",
    "def personal_info_duplicate_classifier(state: AgentState) -> AgentState:\n",
    "    new_info = state.get(\"personal_info_extracted\", \"\")\n",
    "    namespace = (\"memories\", USER_ID)\n",
    "    results = store.search(namespace)\n",
    "    old_info_list = [doc.value[\"data\"] for doc in results]\n",
    "\n",
    "    # Prompt an LLM: \"Haben wir dieses Info-Fragment bereits? Ja/Nein?\"\n",
    "    system_msg = \"\"\"\n",
    "You are a classifier that checks if the new personal info is already stored or not.\n",
    "If the new info adds anything new, respond 'Yes'. If it is essentially a duplicate, respond 'No'.\n",
    "\"\"\"\n",
    "    # Wir fassen alte Info in einem String\n",
    "    old_info_str = \"\\n\".join(old_info_list) if old_info_list else \"No stored info so far.\"\n",
    "\n",
    "    human_template = \"\"\"New info:\\n{new_info}\\n\\nExisting memory:\\n{old_info}\\n\n",
    "Answer ONLY 'Yes' if the new info is unique. Otherwise 'No'.\"\"\"\n",
    "    human_msg = human_template.format(new_info=new_info, old_info=old_info_str)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_msg),\n",
    "            (\"human\", \"{human_msg}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\").with_structured_output(InfoNoveltyGrade)\n",
    "    chain = prompt | llm\n",
    "    result = chain.invoke({\"human_msg\": human_msg})\n",
    "    state[\"is_duplicate\"] = result.score.strip()\n",
    "    return state\n",
    "\n",
    "def personal_info_deduper_router(state: AgentState) -> Literal[\"personal_info_storer\", \"retrieve_memories\"]:\n",
    "    # \"Yes\" => neu => Speichern\n",
    "    # \"No\"  => duplikat => skip\n",
    "    if state[\"is_duplicate\"].lower() == \"yes\":\n",
    "        return \"personal_info_storer\"\n",
    "    return \"retrieve_memories\"\n",
    "\n",
    "#\n",
    "# 4) Speichern (nur falls NEU)\n",
    "#\n",
    "def personal_info_storer(state: AgentState) -> AgentState:\n",
    "    if state.get(\"personal_info_extracted\"):\n",
    "        namespace = (\"memories\", USER_ID)\n",
    "        store.put(namespace, str(uuid.uuid4()), {\"data\": state[\"personal_info_extracted\"]})\n",
    "    return state\n",
    "\n",
    "#\n",
    "# 5) Memories abrufen\n",
    "#\n",
    "def retrieve_memories(state: AgentState) -> AgentState:\n",
    "    namespace = (\"memories\", USER_ID)\n",
    "    results = store.search(namespace)\n",
    "    memory_strs = [doc.value[\"data\"] for doc in results]\n",
    "    state[\"collected_memories\"] = \"\\n\".join(memory_strs)\n",
    "    return state\n",
    "\n",
    "#\n",
    "# 6) Debug-Print\n",
    "#\n",
    "def debug_print_personal_memory(state: AgentState) -> AgentState:\n",
    "    print(\"=== Personal Memory so far ===\")\n",
    "    if state[\"collected_memories\"]:\n",
    "        for i, line in enumerate(state[\"collected_memories\"].split(\"\\n\"), start=1):\n",
    "            print(f\"Memory {i}: {line}\")\n",
    "    else:\n",
    "        print(\"No personal info stored yet.\")\n",
    "    return state\n",
    "\n",
    "#\n",
    "# 7) Finaler LLM-Call\n",
    "#\n",
    "def call_model(state: AgentState) -> AgentState:\n",
    "    personal_info = state.get(\"collected_memories\", \"\")\n",
    "    system_msg = SystemMessage(\n",
    "        content=f\"You are a helpful assistant. The user has shared these personal details:\\n{personal_info}\"\n",
    "    )\n",
    "    all_messages = [system_msg] + list(state[\"messages\"])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    response = llm.invoke(all_messages)\n",
    "    state[\"messages\"] = state[\"messages\"] + [response]\n",
    "    return state\n",
    "\n",
    "\n",
    "#\n",
    "# GRAPH-DEFINITION\n",
    "#\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"personal_info_classifier\", personal_info_classifier)\n",
    "workflow.add_node(\"personal_info_extractor\", personal_info_extractor)\n",
    "workflow.add_node(\"personal_info_duplicate_classifier\", personal_info_duplicate_classifier)\n",
    "workflow.add_node(\"personal_info_storer\", personal_info_storer)\n",
    "workflow.add_node(\"retrieve_memories\", retrieve_memories)\n",
    "workflow.add_node(\"debug_print_personal_memory\", debug_print_personal_memory)\n",
    "workflow.add_node(\"call_model\", call_model)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"personal_info_classifier\",\n",
    "    personal_info_router,\n",
    "    {\n",
    "        \"extract_personal_info\": \"personal_info_extractor\",\n",
    "        \"retrieve_memories\": \"retrieve_memories\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Nach dem Extrahieren => Duplicate-Check => Speichern ODER skip\n",
    "workflow.add_edge(\"personal_info_extractor\", \"personal_info_duplicate_classifier\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"personal_info_duplicate_classifier\",\n",
    "    personal_info_deduper_router,\n",
    "    {\n",
    "        \"personal_info_storer\": \"personal_info_storer\",\n",
    "        \"retrieve_memories\": \"retrieve_memories\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"personal_info_storer\", \"retrieve_memories\")\n",
    "workflow.add_edge(\"retrieve_memories\", \"debug_print_personal_memory\")\n",
    "workflow.add_edge(\"debug_print_personal_memory\", \"call_model\")\n",
    "workflow.add_edge(\"call_model\", END)\n",
    "\n",
    "workflow.set_entry_point(\"personal_info_classifier\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Personal Memory so far ===\n",
      "Memory 1: Thomas Müller enjoys playing football.\n",
      "Memory 2: Thomas Müller enjoys playing football.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Hi, I'm Thomas Müller and I love playing football. Can you suggest something for dinner?\", additional_kwargs={}, response_metadata={}, id='ed77e239-5a55-4354-8e71-ebe762763fe8'),\n",
       "  AIMessage(content='Hi Thomas! Since you enjoy playing football, how about a dinner that’s both delicious and nutritious to fuel your active lifestyle? Here are a few suggestions:\\n\\n1. **Grilled Chicken with Quinoa Salad**: Grilled chicken breast served over a bed of quinoa mixed with cherry tomatoes, cucumber, bell peppers, and a light vinaigrette.\\n\\n2. **Pasta Primavera**: Whole grain pasta tossed with fresh vegetables like zucchini, bell peppers, and spinach, drizzled with olive oil and topped with parmesan cheese.\\n\\n3. **Salmon with Sweet Potato**: Baked salmon served with roasted sweet potatoes and steamed broccoli. It’s packed with protein and healthy fats.\\n\\n4. **Stir-Fried Tofu and Vegetables**: Tofu stir-fried with a mix of your favorite vegetables, served over brown rice or whole grain noodles for a plant-based option.\\n\\n5. **Turkey Tacos**: Ground turkey seasoned with spices, served in whole grain tortillas with avocado, salsa, and lots of veggies.\\n\\nThese meals are not only tasty but also provide the energy you need for your football activities. Enjoy your dinner!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 230, 'prompt_tokens': 49, 'total_tokens': 279, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f2cd28694a', 'finish_reason': 'stop', 'logprobs': None}, id='run-f5da841a-0eea-4a1f-8d81-2bff586eaea5-0', usage_metadata={'input_tokens': 49, 'output_tokens': 230, 'total_tokens': 279, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content=\"Hi, I'm Thomas Müller and I love playing football. Can you suggest something for dinner?\", additional_kwargs={}, response_metadata={}, id='b4163606-ca21-4e7c-9ae4-b65687847e88'),\n",
       "  AIMessage(content=\"Hi Thomas! Since you love playing football, you'll want a dinner that’s nutritious and energizing. Here are a few ideas:\\n\\n1. **Chicken Stir-Fry**: Quick and packed with protein! Sauté chicken breast with a mix of colorful vegetables like bell peppers, broccoli, and carrots. Serve it over brown rice or quinoa.\\n\\n2. **Beef Tacos**: Use lean ground beef or turkey, and serve it in whole-grain tortillas with toppings like lettuce, tomatoes, avocado, and salsa. It’s a fun and tasty option!\\n\\n3. **Mediterranean Bowl**: Combine grilled chicken or chickpeas with quinoa, cherry tomatoes, cucumbers, olives, and feta cheese, drizzled with olive oil and lemon juice.\\n\\n4. **Pasta with Spinach and Ricotta**: Whole grain pasta tossed with fresh spinach, ricotta cheese, garlic, and a sprinkle of parmesan for a creamy, satisfying meal.\\n\\n5. **Baked Salmon**: Serve with a side of roasted sweet potatoes and asparagus. Salmon is rich in omega-3 fatty acids, which are great for recovery.\\n\\nThese meals will help keep you fueled for your football activities. Enjoy your dinner!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 243, 'prompt_tokens': 310, 'total_tokens': 553, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f2cd28694a', 'finish_reason': 'stop', 'logprobs': None}, id='run-22b18e9b-b33c-48e4-8966-cee9859ae871-0', usage_metadata={'input_tokens': 310, 'output_tokens': 243, 'total_tokens': 553, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'personal_info_detected': 'Yes',\n",
       " 'personal_info_extracted': 'Thomas Müller enjoys playing football.',\n",
       " 'collected_memories': 'Thomas Müller enjoys playing football.\\nThomas Müller enjoys playing football.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_1 = {\"messages\": [HumanMessage(content=\"Hi, I'm Thomas Müller and I love playing football. Can you suggest something for dinner?\")]}\n",
    "graph.invoke(input=input_data_1, config={\"configurable\": {\"thread_id\": 99}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_2 = {\"messages\": [HumanMessage(content=\"What do you know about me?\")]}\n",
    "graph.invoke(input=input_data_2, config={\"configurable\": {\"thread_id\": 2}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
